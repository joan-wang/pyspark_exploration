{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "- LEHD Origin-Destination Employment Statistics (LODES): The definition of variable codes, datasets, etc. can be found at the latest [LODES 7.3 Technical Documentation](https://lehd.ces.census.gov/data/lodes/LODES7/LODESTechDoc7.3.pdf). All LEHD Origin-Destination Employment Statistics (LODES) data are available, as described in the LODES documentation above. No changes have been made to the original CSV files. Data are available from 2002 to 2015. See the documentation above for caveats.\n",
    "- Driving Times and Distances Dataset: Census tracts are 2010 vintage, and the columns are the origin tract, destination travel, travel time in minutes, and travel distance in miles. These data were calculated by the Data Science team at the Urban Institute. See [Github repo](https://github.com/UI-Research/spark-osrm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('pyspark-exploration') \\\n",
    "    .config('spark.driver.cores', '2') \\\n",
    "    .config('spark.executor.memory', '8gb') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .getOrCreate()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(df):\n",
    "    \"\"\"\n",
    "    Function to pretty print the toDebugString\n",
    "    \"\"\"\n",
    "    for rddstring in df.rdd.toDebugString().split('\\n'):\n",
    "        print rddstring.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = spark.read.parquet('s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet')\n",
    "od = spark.read.parquet('s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122004331, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(from_tract=u'36103146402', to_tract=u'42091207003', miles=141.2, minutes=184.1),\n",
       " Row(from_tract=u'36103146402', to_tract=u'42091209000', miles=162.7, minutes=203.4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('from_tract', 'string'),\n",
       " ('to_tract', 'string'),\n",
       " ('miles', 'double'),\n",
       " ('minutes', 'double')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((drive.count(), len(drive.columns)))\n",
    "drive.take(2)\n",
    "drive.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1577789908, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(w_geocode=u'271630714002025', h_geocode=u'271630712082020', s000=1, sa01=0, sa02=1, sa03=0, se01=0, se02=1, se03=0, si01=1, si02=0, si03=0, createdate=u'20160219', year=2012),\n",
       " Row(w_geocode=u'271630714002025', h_geocode=u'271630712083004', s000=1, sa01=0, sa02=1, sa03=0, se01=0, se02=1, se03=0, si01=1, si02=0, si03=0, createdate=u'20160219', year=2012)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('w_geocode', 'string'),\n",
       " ('h_geocode', 'string'),\n",
       " ('s000', 'int'),\n",
       " ('sa01', 'int'),\n",
       " ('sa02', 'int'),\n",
       " ('sa03', 'int'),\n",
       " ('se01', 'int'),\n",
       " ('se02', 'int'),\n",
       " ('se03', 'int'),\n",
       " ('si01', 'int'),\n",
       " ('si02', 'int'),\n",
       " ('si03', 'int'),\n",
       " ('createdate', 'string'),\n",
       " ('year', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((od.count(), len(od.columns)))\n",
    "od.take(2)\n",
    "od.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make census tract and state columns in origin-destination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = od.withColumn('h_tract', substring(od.h_geocode, 0, 11))\\\n",
    "        .withColumn('w_tract', substring(od.w_geocode, 0, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = od.withColumn('h_state', substring(od.h_geocode, 0, 2))\\\n",
    "        .withColumn('w_state', substring(od.w_geocode, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make state column in drive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = drive.withColumn('from_state', substring(drive.from_tract, 0, 2))\\\n",
    "            .withColumn('to_state', substring(drive.to_tract, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make \"total\" and \"pct\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_cols = ['sa01', 'sa02', 'sa03']\n",
    "se_cols = ['se01', 'se02', 'se03']\n",
    "si_cols = ['si01', 'si02', 'si03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = od.withColumn('sa_total', od.sa01+od.sa02+od.sa03)\\\n",
    "    .withColumn('se_total', od.se01+od.se02+od.se03)\\\n",
    "    .withColumn('si_total', od.si01+od.si02+od.si03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_ls, cat_name in [(sa_cols, 'sa_total'), (se_cols, 'se_total'), (si_cols, 'si_total')]:\n",
    "    for col in cat_ls:\n",
    "        new = col + '_pct'\n",
    "        od = od.withColumn(new, od[col]/od[cat_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w_geocode',\n",
       " 'h_geocode',\n",
       " 's000',\n",
       " 'sa01',\n",
       " 'sa02',\n",
       " 'sa03',\n",
       " 'se01',\n",
       " 'se02',\n",
       " 'se03',\n",
       " 'si01',\n",
       " 'si02',\n",
       " 'si03',\n",
       " 'createdate',\n",
       " 'year',\n",
       " 'h_tract',\n",
       " 'w_tract',\n",
       " 'h_state',\n",
       " 'w_state',\n",
       " 'sa_total',\n",
       " 'se_total',\n",
       " 'si_total',\n",
       " 'sa01_pct',\n",
       " 'sa02_pct',\n",
       " 'sa03_pct',\n",
       " 'se01_pct',\n",
       " 'se02_pct',\n",
       " 'se03_pct',\n",
       " 'si01_pct',\n",
       " 'si02_pct',\n",
       " 'si03_pct']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out lineage and partitions of the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137) MapPartitionsRDD[25] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[24] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[23] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   FileScanRDD[22] at javaToPython at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "debug(od)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33) MapPartitionsRDD[29] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[28] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[27] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   FileScanRDD[26] at javaToPython at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "debug(drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od.rdd.getNumPartitions()\n",
    "drive.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join: origin-destination and driving dataframes  \n",
    "Assumption: travel time and distance for a census tract is the same for all its comprising block groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition od before joining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = od.repartition(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try working with just California data first, where origin and destination are state code 06  \n",
    "Join od_ca and drive_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_ca = od.where(\"h_state == 06\" and \"w_state == 06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_ca = drive.where('to_state == 06' and 'from_state == 06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = od_ca.join(drive_ca, [drive.from_tract == od.h_tract, drive.to_tract == od.w_tract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(w_geocode=u'060014355002011', h_geocode=u'060014001001045', s000=1, sa01=1, sa02=0, sa03=0, se01=0, se02=1, se03=0, si01=0, si02=0, si03=1, createdate=u'20160228', year=2008, h_tract=u'06001400100', w_tract=u'06001435500', h_state=u'06', w_state=u'06', sa_total=1, se_total=1, si_total=1, sa01_pct=1.0, sa02_pct=0.0, sa03_pct=0.0, se01_pct=0.0, se02_pct=1.0, se03_pct=0.0, si01_pct=0.0, si02_pct=0.0, si03_pct=1.0, from_tract=u'06001400100', to_tract=u'06001435500', miles=16.9, minutes=24.2, from_state=u'06', to_state=u'06')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ca.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join full od with driving, giving us travel times for each origin-destination pair.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = od.join(drive, [drive.from_tract == od.h_tract, drive.to_tract == od.w_tract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting dataframe is split across 200 partitions, as we can see from the getNumPartitions method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200) MapPartitionsRDD[60] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[59] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[58] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   ZippedPartitionsRDD2[57] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[51] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   ShuffledRowRDD[50] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "+-(8) MapPartitionsRDD[49] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|  ShuffledRowRDD[48] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "+-(137) MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[46] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   FileScanRDD[45] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[56] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   ShuffledRowRDD[55] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "+-(33) MapPartitionsRDD[54] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   MapPartitionsRDD[53] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "|   FileScanRDD[52] at javaToPython at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "debug(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped aggregation: proportion of goods-producing jobs per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, start with California only  \n",
    "\n",
    "The proportion of jobs in California in Goods Producing industry sectors (\"si01\") has steadily declined over the past decade as the economy moved away from manufacturing and towards services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ca = df_ca.groupBy(\"year\").agg(sum(\"si01\"), sum('si02'), sum('si03'))\n",
    "results_ca = agg_ca.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ca_df = pd.DataFrame(results_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ca_df.columns = ['year', 'si01', 'si02', 'si03']\n",
    "agg_ca_df['total_jobs'] = agg_ca_df.si01 + agg_ca_df.si02 + agg_ca_df.si03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ca_df['pct_si01'] = agg_ca_df.si01 / agg_ca_df.total_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>si01</th>\n",
       "      <th>si02</th>\n",
       "      <th>si03</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>pct_si01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2002</td>\n",
       "      <td>1061707</td>\n",
       "      <td>1033147</td>\n",
       "      <td>3245012</td>\n",
       "      <td>5339866</td>\n",
       "      <td>0.198827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1019401</td>\n",
       "      <td>1024673</td>\n",
       "      <td>3263365</td>\n",
       "      <td>5307439</td>\n",
       "      <td>0.192070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2004</td>\n",
       "      <td>1028667</td>\n",
       "      <td>1022742</td>\n",
       "      <td>3306224</td>\n",
       "      <td>5357633</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2005</td>\n",
       "      <td>1052749</td>\n",
       "      <td>1053289</td>\n",
       "      <td>3372850</td>\n",
       "      <td>5478888</td>\n",
       "      <td>0.192146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>1045599</td>\n",
       "      <td>1071275</td>\n",
       "      <td>3441228</td>\n",
       "      <td>5558102</td>\n",
       "      <td>0.188122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>1025099</td>\n",
       "      <td>1065219</td>\n",
       "      <td>3474935</td>\n",
       "      <td>5565253</td>\n",
       "      <td>0.184196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008</td>\n",
       "      <td>967440</td>\n",
       "      <td>1052308</td>\n",
       "      <td>3553468</td>\n",
       "      <td>5573216</td>\n",
       "      <td>0.173587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009</td>\n",
       "      <td>877083</td>\n",
       "      <td>977576</td>\n",
       "      <td>3534286</td>\n",
       "      <td>5388945</td>\n",
       "      <td>0.162756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>823689</td>\n",
       "      <td>978019</td>\n",
       "      <td>3610732</td>\n",
       "      <td>5412440</td>\n",
       "      <td>0.152184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>827560</td>\n",
       "      <td>1009758</td>\n",
       "      <td>3651098</td>\n",
       "      <td>5488416</td>\n",
       "      <td>0.150783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012</td>\n",
       "      <td>824822</td>\n",
       "      <td>1018812</td>\n",
       "      <td>3702850</td>\n",
       "      <td>5546484</td>\n",
       "      <td>0.148711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>847619</td>\n",
       "      <td>1048702</td>\n",
       "      <td>3813783</td>\n",
       "      <td>5710104</td>\n",
       "      <td>0.148442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>891843</td>\n",
       "      <td>1067155</td>\n",
       "      <td>3908049</td>\n",
       "      <td>5867047</td>\n",
       "      <td>0.152009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>939611</td>\n",
       "      <td>1108226</td>\n",
       "      <td>4070729</td>\n",
       "      <td>6118566</td>\n",
       "      <td>0.153567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year     si01     si02     si03  total_jobs  pct_si01\n",
       "13  2002  1061707  1033147  3245012     5339866  0.198827\n",
       "0   2003  1019401  1024673  3263365     5307439  0.192070\n",
       "6   2004  1028667  1022742  3306224     5357633  0.192000\n",
       "9   2005  1052749  1053289  3372850     5478888  0.192146\n",
       "3   2006  1045599  1071275  3441228     5558102  0.188122\n",
       "1   2007  1025099  1065219  3474935     5565253  0.184196\n",
       "12  2008   967440  1052308  3553468     5573216  0.173587\n",
       "8   2009   877083   977576  3534286     5388945  0.162756\n",
       "10  2010   823689   978019  3610732     5412440  0.152184\n",
       "11  2011   827560  1009758  3651098     5488416  0.150783\n",
       "7   2012   824822  1018812  3702850     5546484  0.148711\n",
       "4   2013   847619  1048702  3813783     5710104  0.148442\n",
       "5   2014   891843  1067155  3908049     5867047  0.152009\n",
       "2   2015   939611  1108226  4070729     6118566  0.153567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_ca_df.sort_values('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it for the whole country. We see a similar pattern of declining percentage of Goods Producing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full = df.groupBy(\"year\").agg(sum(\"si01\"), sum('si02'), sum('si03'))\n",
    "results_full = agg_full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_full_df = pd.DataFrame(results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_df.columns = ['year', 'si01', 'si02', 'si03']\n",
    "agg_full_df['total_jobs'] = agg_full_df.si01 + agg_full_df.si02 + agg_full_df.si03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_df['pct_si01'] = agg_full_df.si01 / agg_full_df.total_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>si01</th>\n",
       "      <th>si02</th>\n",
       "      <th>si03</th>\n",
       "      <th>total_jobs</th>\n",
       "      <th>pct_si01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2002</td>\n",
       "      <td>9049198</td>\n",
       "      <td>9483453</td>\n",
       "      <td>27533384</td>\n",
       "      <td>46066035</td>\n",
       "      <td>0.196440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>8865886</td>\n",
       "      <td>9600409</td>\n",
       "      <td>27942515</td>\n",
       "      <td>46408810</td>\n",
       "      <td>0.191039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2004</td>\n",
       "      <td>9121645</td>\n",
       "      <td>9926406</td>\n",
       "      <td>29220115</td>\n",
       "      <td>48268166</td>\n",
       "      <td>0.188978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2005</td>\n",
       "      <td>9300469</td>\n",
       "      <td>10086742</td>\n",
       "      <td>29813064</td>\n",
       "      <td>49200275</td>\n",
       "      <td>0.189033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>9447083</td>\n",
       "      <td>10167374</td>\n",
       "      <td>30462147</td>\n",
       "      <td>50076604</td>\n",
       "      <td>0.188653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>9357236</td>\n",
       "      <td>10167600</td>\n",
       "      <td>30749160</td>\n",
       "      <td>50273996</td>\n",
       "      <td>0.186125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008</td>\n",
       "      <td>9023304</td>\n",
       "      <td>10151678</td>\n",
       "      <td>31044813</td>\n",
       "      <td>50219795</td>\n",
       "      <td>0.179676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009</td>\n",
       "      <td>7960944</td>\n",
       "      <td>9624079</td>\n",
       "      <td>30834938</td>\n",
       "      <td>48419961</td>\n",
       "      <td>0.164415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>7515555</td>\n",
       "      <td>9468287</td>\n",
       "      <td>31314073</td>\n",
       "      <td>48297915</td>\n",
       "      <td>0.155608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>7827414</td>\n",
       "      <td>9978429</td>\n",
       "      <td>32872199</td>\n",
       "      <td>50678042</td>\n",
       "      <td>0.154454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012</td>\n",
       "      <td>7968824</td>\n",
       "      <td>10138958</td>\n",
       "      <td>33284584</td>\n",
       "      <td>51392366</td>\n",
       "      <td>0.155059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>8055568</td>\n",
       "      <td>10244986</td>\n",
       "      <td>33826994</td>\n",
       "      <td>52127548</td>\n",
       "      <td>0.154536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>8356186</td>\n",
       "      <td>10390904</td>\n",
       "      <td>34407571</td>\n",
       "      <td>53154661</td>\n",
       "      <td>0.157205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>8599917</td>\n",
       "      <td>10707646</td>\n",
       "      <td>35311446</td>\n",
       "      <td>54619009</td>\n",
       "      <td>0.157453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year     si01      si02      si03  total_jobs  pct_si01\n",
       "13  2002  9049198   9483453  27533384    46066035  0.196440\n",
       "0   2003  8865886   9600409  27942515    46408810  0.191039\n",
       "6   2004  9121645   9926406  29220115    48268166  0.188978\n",
       "9   2005  9300469  10086742  29813064    49200275  0.189033\n",
       "3   2006  9447083  10167374  30462147    50076604  0.188653\n",
       "1   2007  9357236  10167600  30749160    50273996  0.186125\n",
       "12  2008  9023304  10151678  31044813    50219795  0.179676\n",
       "8   2009  7960944   9624079  30834938    48419961  0.164415\n",
       "10  2010  7515555   9468287  31314073    48297915  0.155608\n",
       "11  2011  7827414   9978429  32872199    50678042  0.154454\n",
       "7   2012  7968824  10138958  33284584    51392366  0.155059\n",
       "4   2013  8055568  10244986  33826994    52127548  0.154536\n",
       "5   2014  8356186  10390904  34407571    53154661  0.157205\n",
       "2   2015  8599917  10707646  35311446    54619009  0.157453"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_full_df.sort_values('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window function: average total jobs over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"h_geocode\", \"w_geocode\")\\\n",
    "            .orderBy(\"year\")\\\n",
    "            .rowsBetween(Window.currentRow -1, Window.currentRow + 1)\n",
    "avg_jobs = avg(col('s000')).over(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca.select('year', 'h_geocode', 'w_geocode', avg_jobs.alias(\"avg_jobs\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"h_geocode\", \"w_geocode\")\\\n",
    "        .orderBy('year')\\\n",
    "        .rangeBetween(-1, 1)\n",
    "\n",
    "df_ca = df_ca.withColumn('centered_avg_jobs', avg(\"s000\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(w_geocode=u'060014040002005', h_geocode=u'060014001001007', s000=1, sa01=0, sa02=0, sa03=1, se01=0, se02=0, se03=1, si01=0, si02=0, si03=1, createdate=u'20160228', year=2005, h_tract=u'06001400100', w_tract=u'06001404000', h_state=u'06', w_state=u'06', sa_total=1, se_total=1, si_total=1, sa01_pct=0.0, sa02_pct=0.0, sa03_pct=1.0, se01_pct=0.0, se02_pct=0.0, se03_pct=1.0, si01_pct=0.0, si02_pct=0.0, si03_pct=1.0, from_tract=u'06001400100', to_tract=u'06001404000', miles=4.2, minutes=11.5, from_state=u'06', to_state=u'06', centered_avg_jobs=1.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ca.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data from long to wide based on year column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try it for California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[w_geocode: string, h_geocode: string, s000: int, sa01: int, sa02: int, sa03: int, se01: int, se02: int, se03: int, si01: int, si02: int, si03: int, createdate: string, year: int, h_tract: string, w_tract: string, h_state: string, w_state: string, sa_total: int, se_total: int, si_total: int, sa01_pct: double, sa02_pct: double, sa03_pct: double, se01_pct: double, se02_pct: double, se03_pct: double, si01_pct: double, si02_pct: double, si03_pct: double, from_tract: string, to_tract: string, miles: double, minutes: double, from_state: string, to_state: string, centered_avg_jobs: double]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ca.na.drop('all', subset=['h_geocode', 'w_geocode', 'year', 's000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o444.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_geocode#10, w_geocode#9, 200)\n+- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n   :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n   :     +- Exchange RoundRobinPartitioning(8)\n   :        +- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n   :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n   :              +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n   +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n         +- *Project [from_tract#0, to_tract#1, miles#2, minutes#3, substring(from_tract#0, 0, 2) AS from_state#146, substring(to_tract#1, 0, 2) AS to_state#153]\n            +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n               +- *FileScan parquet [from_tract#0,to_tract#1,miles#2,minutes#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string,miles:double,minutes:double>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.window.WindowExec.doExecute(WindowExec.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2808)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2805)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2805)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2828)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2805)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_tract#72, w_tract#89, 200)\n+- Exchange RoundRobinPartitioning(8)\n   +- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n      +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:377)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 43 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange RoundRobinPartitioning(8)\n+- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n   +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n      +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 78 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:236)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:214)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1485)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:333)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:285)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:283)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:303)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:124)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 89 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c6a6e44a7d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \"\"\"\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o444.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_geocode#10, w_geocode#9, 200)\n+- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n   :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n   :     +- Exchange RoundRobinPartitioning(8)\n   :        +- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n   :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n   :              +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n   +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n         +- *Project [from_tract#0, to_tract#1, miles#2, minutes#3, substring(from_tract#0, 0, 2) AS from_state#146, substring(to_tract#1, 0, 2) AS to_state#153]\n            +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n               +- *FileScan parquet [from_tract#0,to_tract#1,miles#2,minutes#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string,miles:double,minutes:double>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.window.WindowExec.doExecute(WindowExec.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2808)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2805)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2805)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2828)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2805)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_tract#72, w_tract#89, 200)\n+- Exchange RoundRobinPartitioning(8)\n   +- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n      +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:377)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 43 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange RoundRobinPartitioning(8)\n+- *Project [w_geocode#9, h_geocode#10, s000#11, sa01#12, sa02#13, sa03#14, se01#15, se02#16, se03#17, si01#18, si02#19, si03#20, createdate#21, year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89, substring(h_geocode#10, 0, 2) AS h_state#107, substring(w_geocode#9, 0, 2) AS w_state#126, ((sa01#12 + sa02#13) + sa03#14) AS sa_total#161, ((se01#15 + se02#16) + se03#17) AS se_total#182, ((si01#18 + si02#19) + si03#20) AS si_total#204, (cast(sa01#12 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa01_pct#227, (cast(sa02#13 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa02_pct#251, (cast(sa03#14 as double) / cast(((sa01#12 + sa02#13) + sa03#14) as double)) AS sa03_pct#276, ... 6 more fields]\n   +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n      +- *FileScan parquet [w_geocode#9,h_geocode#10,s000#11,sa01#12,sa02#13,sa03#14,se01#15,se02#16,se03#17,si01#18,si02#19,si03#20,createdate#21,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,s000:int,sa01:int,sa02:int,sa03:int,se01:int,se02:int,se...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 78 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:236)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:214)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1485)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:333)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:285)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:283)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:303)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:124)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 89 more\n"
     ]
    }
   ],
   "source": [
    "df_ca.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o401.pivot.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange rangepartitioning(year#22 ASC NULLS FIRST, 200)\n+- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n   +- Exchange hashpartitioning(year#22, 200)\n      +- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n         +- *Project [year#22]\n            +- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n               :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n               :     +- Exchange RoundRobinPartitioning(8)\n               :        +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n               :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n               :              +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n               +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n                     +- *Project [from_tract#0, to_tract#1]\n                        +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n                           +- *FileScan parquet [from_tract#0,to_tract#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2586)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2583)\n\tat org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(year#22, 200)\n+- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n   +- *Project [year#22]\n      +- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n         :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n         :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n         :     +- Exchange RoundRobinPartitioning(8)\n         :        +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n         :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         :              +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n         +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n               +- *Project [from_tract#0, to_tract#1]\n                  +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n                     +- *FileScan parquet [from_tract#0,to_tract#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_tract#72, w_tract#89, 200)\n+- Exchange RoundRobinPartitioning(8)\n   +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n      +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:377)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 59 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange RoundRobinPartitioning(8)\n+- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n   +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n      +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 96 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:236)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:214)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1485)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:333)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:285)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:283)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:303)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:124)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 107 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8f2b3b16fca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpivoted1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h_geocode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w_geocode'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, pivot_col, values)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o401.pivot.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange rangepartitioning(year#22 ASC NULLS FIRST, 200)\n+- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n   +- Exchange hashpartitioning(year#22, 200)\n      +- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n         +- *Project [year#22]\n            +- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n               :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n               :     +- Exchange RoundRobinPartitioning(8)\n               :        +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n               :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n               :              +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n               +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n                     +- *Project [from_tract#0, to_tract#1]\n                        +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n                           +- *FileScan parquet [from_tract#0,to_tract#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2586)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2583)\n\tat org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(year#22, 200)\n+- *HashAggregate(keys=[year#22], functions=[], output=[year#22])\n   +- *Project [year#22]\n      +- *SortMergeJoin [h_tract#72, w_tract#89], [from_tract#0, to_tract#1], Inner\n         :- *Sort [h_tract#72 ASC NULLS FIRST, w_tract#89 ASC NULLS FIRST], false, 0\n         :  +- Exchange hashpartitioning(h_tract#72, w_tract#89, 200)\n         :     +- Exchange RoundRobinPartitioning(8)\n         :        +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n         :           +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         :              +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n         +- *Sort [from_tract#0 ASC NULLS FIRST, to_tract#1 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(from_tract#0, to_tract#1, 200)\n               +- *Project [from_tract#0, to_tract#1]\n                  +- *Filter ((isnotnull(from_tract#0) && (cast(substring(from_tract#0, 0, 2) as int) = 6)) && isnotnull(to_tract#1))\n                     +- *FileScan parquet [from_tract#0,to_tract#1] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(from_tract), IsNotNull(to_tract)], ReadSchema: struct<from_tract:string,to_tract:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(h_tract#72, w_tract#89, 200)\n+- Exchange RoundRobinPartitioning(8)\n   +- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n      +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n         +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:244)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:377)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 59 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange RoundRobinPartitioning(8)\n+- *Project [year#22, substring(h_geocode#10, 0, 11) AS h_tract#72, substring(w_geocode#9, 0, 11) AS w_tract#89]\n   +- *Filter (((((isnotnull(w_geocode#9) && (cast(substring(w_geocode#9, 0, 2) as int) = 6)) && isnotnull(h_geocode#10)) && isnotnull(substring(h_geocode#10, 0, 11))) && (cast(substring(substring(h_geocode#10, 0, 11), 0, 2) as int) = 6)) && isnotnull(substring(w_geocode#9, 0, 11)))\n      +- *FileScan parquet [w_geocode#9,h_geocode#10,year#22] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(w_geocode), IsNotNull(h_geocode)], ReadSchema: struct<w_geocode:string,h_geocode:string,year:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 96 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:236)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:214)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1485)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:333)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:285)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:283)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:303)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:124)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 107 more\n"
     ]
    }
   ],
   "source": [
    "pivoted1 = df_ca.groupby('h_geocode', 'w_geocode')\\\n",
    "    .pivot(\"year\")\\\n",
    "    .sum(\"s000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df.groupby('h_geocode', 'w_geocode')\\\n",
    "    .pivot(\"year\")\\\n",
    "    .sum(\"s000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: How does age and earnings affect distance traveled for work?\n",
    "\n",
    "Regression equation: dist = b0 + b1&ast;sa02_pct + b2&ast;sa03_pct + b3&ast;se02_pct + b4&ast;se03_pct + error  \n",
    "sa02 = % of jobs for workers age 30 to 54  \n",
    "sa03 = % of jobs for workers age 55 or older  \n",
    "se02 = % of jobs with earnings 1251/month to 3333/month \n",
    "se03 = % of jobs with earnings greater than 3333/month\n",
    "\n",
    "Note: first category of each group (age an earnings) are left out of equation to serve as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca_reg = df_ca[['sa02_pct', 'sa03_pct', 'se02_pct', 'se03_pct', 'miles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca_reg_vect = df_ca_reg.rdd.map(lambda x: [Vectors.dense(x[0:4]), x[-1]]).toDF(['features', 'label'])\n",
    "df_ca_reg_vect.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
